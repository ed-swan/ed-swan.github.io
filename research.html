<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="css/responsive.css" type="text/css" media="all">
    <title>J. Edward Swan II</title>
  </head>

  <body>
    <div class="outer_wrapper">
      <div class="inner_wrapper">

        <div class="header">
          <p>J. Edward Swan II</p>
        </div>

        <div class="menu">
          <ul>
            <li><a href="index.html"><em>A</em>BOUT</a></li>&nbsp;&bullet;
            <li><a href="research.html" class="selected"><em>R</em>ESEARCH</a></li>&nbsp;&bullet;
            <li><a href="publications/sort_date.html"><em>P</em>UBLICATIONS</a></li>&nbsp;&bullet;
            <li><a href="teaching.html"><em>T</em>EACHING</a></li>&nbsp;&bullet;
            <li><a href="tutorials.html"><em>T</em>UTORIALS</a></li>&nbsp;&bullet;
            <li><a href="students.html"><em>S</em>TUDENTS</a></li>
          </ul>
          <p>&nbsp;</p>
        </div>

        <div class="content">
          <h1>Laboratory Manifest</h1>
          
          <p>The <em>Spatial Perception And Augmented Reality Lab</em>
            (SP<sup><u>AA</u></sup>R Lab) studies the perception and technology
            required to give <em>virtual objects definite spatial
            locations</em>.&nbsp; <i>Projects</i> involve perception,
            measurement, calibration, and display
            technologies.&nbsp; <i>Methods</i> are interdisciplinary, including
            computer science, empirical methods, psychology, cognitive science,
            optics, and engineering.&nbsp; <i>Culture</i> involves teamwork, and
            the pursuit of beauty through scholarship and intellectual merit of
            the highest possible quality.</p>
          
          <h1>Current Research</h1>

          <p>My research can be most broadly described as conducting
            human-factors investigations of computer-generated graphics.&nbsp;
            I have primarily focused on the areas of Augmented Reality, Virtual
            Reality, and Visualization.&nbsp; Problem domains have included
            surgical simulation and other medical applications, weather
            visualization, military applications, computer forensics, flow-field
            visualization, and driving simulation.</p>

          <h2>Perception in Augmented and Virtual Reality</h2>

          <p>At MSU, I have founded the <em>Spatial Perception And Augmented Reality
            Lab</em> (SP<sup><u>AA</u></sup>R Lab), where we study perceptual
            aspects of Augmented and Virtual Reality, with an emphasis on the
            knowledge of, and methods for, presenting virtual objects that have
            definite spatial locations.&nbsp; This work encompasses studying how
            <em>depth perception</em> operates in augmented and virtual reality,
            as well as <em>x-ray vision</em>: how one can use augmented reality
            to let users see objects which are located behind solid, opaque
            surfaces&mdash;to allow, for example, a doctor to see organs that
            are inside a patient's body.&nbsp; An important related area has
            been the augmented reality <em>calibration methods</em> that allow
            accurate virtual object positioning.  This work has involved many
            collaborations.&nbsp; In addition to my students, for more than 10
            years I have collaborated with Stephen R. Ellis (NASA Ames Research
            Center); other collaborators have come from the University of
            Southern California, the University of South Australia, the Nara
            Institute of Science and Technology, the University of
            Jyv&auml;skyl&auml;, the Naval Research Laboratory, NVIDIA
            Corporation, the University of California at Santa Barbara, and the
            University of Rostock.&nbsp; The work has been funded by the
            National Science Foundation, the National Aeronautics and Space
            Administration (NASA), the Department of Defense, the Naval Research
            Laboratory, and the Office of Naval Research.</p>

          <h2>Human-Factors Aspects of Augmented and Virtual Reality</h2>

          <p>In addition to virtual object location, I have collaborated widely
            with colleagues and students on other human-factors aspects of
            augmented and virtual reality.&nbsp; Projects have involved the
            effect of switching context between augmented reality and the real
            world, how color and contrast perception operate in optical
            see-through augmented reality, how gait changes in virtual
            environments, driving simulator studies of cell phone distraction,
            and general reviews of perceptual issues.&nbsp; My longest
            collaboration, which has covered a number of these topics, has been
            with Joe Gabbard at Virginia Tech.  In addition, with Joe and Debby
            Hix, also from Virginia Tech, I helped articulate some of the first
            systematic usability engineering concepts for augmented reality
            (IEEE Transactions on Visualization and Computer Graphics, 2008;
            IEEE Computer Graphics and Applications, 1999).&nbsp; This work has
            been funded by the National Science Foundation, the Department of
            Defense, the Naval Research Laboratory, and the Office of Naval
            Research.</p>

          <h2>Visualization and Evaluation</h2>

          <p>I have also worked with students and collaborators on a number of
            visualization and evaluation projects.&nbsp; These have involved
            visualizing and interacting with ensembles of weather model
            simulation runs, visualizing computer forensics data, cognitively
            evaluating the effectiveness of forensics data visualizations, using
            parallel coordinates to visualize historical hurricane severity
            data, empirically evaluating additional weather data visualization
            techniques, empirically evaluating tensor visualization methods, and
            empirically evaluating flow-field visualization techniques.&nbsp;
            This work has involved collaborations with many MSU colleagues, in
            particular Song Zhang, T.J. Jankun-Kelly, Andrew Mercer, Jamie Dyer,
            and a number of students.&nbsp; This work has been funded by the
            National Science Foundation, Department of the Army, and the Naval
            Research Laboratory.</p>

          <h1>Previous Research Topics</h1>

          <p>I was employed by the Naval Research Lab (NRL) from 1997 through
            2004.&nbsp; While there, I was a member of the Virtual Reality
            Laboratory.&nbsp; Our primary project during this time was
            researching and developing the Battlefield Augmented Reality System
            (BARS), an outdoor, mobile augmented reality system that allowed
            soldiers on foot to see heads-up, graphically-drawn battlefield
            information through optical see-through displays.&nbsp; Anyone who
            has worked in augmented and virtual reality can appreciate how
            outrageously challenging this project was, especially considering
            the technical maturity of displays and computers during that
            era.&nbsp; I had a great time and learned a huge amount.&nbsp; My
            experiences with BARS are what lead me to originally become
            interested in the perceptual issues that arise from augmented and
            virtual reality.&nbsp; This work was a team effort with my fellow
            NRL scientists Larry Rosenblum (my boss), Simon Julier, Mark
            Livingston, Greg Schmidt, Yohan Baillot, and Dennis Brown.&nbsp; In
            addition, much of my work during this time also involved
            collaborations with my Virginia Tech colleagues Joe Gabbard and
            Deborah Hix.</p>

          <p>From 1994 through 1997 I was engaged in my PhD studies in the areas
            of Volume Rendering, Visualization, and Graphics; my PhD adviser was
            Roni Yagel.&nbsp; With the help of Roni and my fellow PhD students
            Klaus Mueller and Torsten M&ouml;ller, I developed the first
            published algorithm (IEEE Visualization 1997) for anti-aliasing with
            Splatting.&nbsp; Since that time, many others have greatly advanced
            this early work (including Klaus and Torsten).</p>

          <p>From 1992 through 1994 I worked with my MS adviser Gary Perlman in
            the area of Human-Computer Interaction.&nbsp; Under Gary's direction
            I conducted two perceptual user studies (Proceedings of the Human
            Factors and Ergonomics Society, 1993 and 1994).&nbsp; Although these
            were my very first research experiences, I find that I still often
            apply the lessons and knowledge that I first learned from Gary.</p>
        </div>
      </div>
    </div>
  </body>
</html>
