<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="StyleSheet" href="../css/responsive.css" type="text/css" media="all">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>J. Edward Swan II</title>
</head>
<body><div class="outer_wrapper"><div class="inner_wrapper">
<div class="header"><p>J. Edward Swan II</p></div>
<div class="menu">
<ul>
<li><a href="../index.html"><em>A</em>BOUT</a></li> •
           <li><a href="../research.html"><em>R</em>ESEARCH</a></li> •
           <li><a href="sort_date.html" class="selected"><em>P</em>UBLICATIONS</a></li> •
           <li><a href="../teaching.html"><em>T</em>EACHING</a></li> •
           <li><a href="../tutorials.html"><em>T</em>UTORIALS</a></li> •
           <li><a href="../students.html"><em>S</em>TUDENTS</a></li>
</ul>
<p> </p>
</div>
<div class="content">
<ul class="h_list">
<li><big>• <a href="class_rescat.html">Classified by Research Category</a> </big></li>
<li><big>• <a href="class_type.html">Classified by Publication Type</a> </big></li>
<li><big>• <a href="sort_date.html">Classified by Date</a> </big></li>
</ul>
<h2> Tracking Perceptual Depth with Eye Vergence Movements in Real World,                 Augmented Reality, and Virtual Reality Environments</h2>
<p class="citation"> Mohammed Safayet Arefin,  J. Edward Swan&nbsp;II,  Russell Cohen-Hoffing, and  Steven M. Thurman.  Tracking Perceptual Depth with Eye Vergence Movements in Real World,                 Augmented Reality, and Virtual Reality Environments. In <i> Vision Sciences Society Annual Meeting Abstract (VSS), Journal of Vision</i>,  23, Aug 2023.  DOI: <a target="_blank"                 href="https://doi.org/10.1167/jov.23.9.5209">10.1167/jov.23.9.5209</a>.</p>
<h3>Download</h3>
<p>
  (unavailable)
  </p>
<h3>Abstract</h3>
<p class="abstract"> Completing tasks in the real world, augmented reality (AR) and virtual reality (VR) require users to integrate visual information located at different distances. Eye movements like vergence and accommodation are engaged to render objects and visual information in sharp focus. However, due to the vergence-accommodation conflict inherent to artificial displays, it is unclear whether depth related eye movements in AR and VR unfold similarly to the real world. Our study measured eye vergence continuously to track perceptual depth changes with an eye tracker in the visually-matched real world, AR, and VR conditions. Participants were cued to shift their gaze to targets at one of four different depths (0.25m, 0.75m, 1.5m, and 4.0m) and completed an alternative forced choice perceptual task reporting the direction of the gap in a Landolt C stimulus. Trial order was fully counterbalanced. Four physical monitors were placed at the experimental depths to present the real-world stimuli on-screen. We custom-mounted the pupil labs eye tracker on Microsoft HoloLens 2 to collect the eye tracker data and represent stimuli in AR and VR. In the VR condition, Microsoft HoleLens 2's optics were covered with a black cloth, so no light entered. As expected, results showed that eye vergence angle increased measurably when gaze-shifting far-to-near and decreased when shifting near-to-far. Importantly, all three conditions induced a similar exponential function by which the vergence angle was modulated by depth regardless of whether the targets were presented on real-world displays or head-mounted displays. </p>
<a href="VSS23.bib"><h3>BibTeX</h3></a><pre>@InProceedings{VSS23, 
  author =      {Mohammed Safayet Arefin and J. Edward {Swan~II} and Russell Cohen-Hoffing
                 and Steven M. Thurman},
  title =       {Tracking Perceptual Depth with Eye Vergence Movements in Real World,
                 Augmented Reality, and Virtual Reality Environments},
  booktitle =   {Vision Sciences Society Annual Meeting Abstract (VSS), Journal of Vision},
  location =    {St. Pete Beach, Florida, USA}, 
  date =        {May 19--24},
  volume =      {23},
  page =        {5209},
  month =       {Aug}, 
  year =        2023,
  note =        {DOI: &lt;a target="_blank"
                 href="https://doi.org/10.1167/jov.23.9.5209"&gt;10.1167/jov.23.9.5209&lt;/a&gt;.},
  abstract = {
Completing tasks in the real world, augmented reality (AR) and virtual reality 
(VR) require users to integrate visual information located at different 
distances. Eye movements like vergence and accommodation are engaged to render 
objects and visual information in sharp focus. However, due to the 
vergence-accommodation conflict inherent to artificial displays, it is unclear 
whether depth related eye movements in AR and VR unfold similarly to the real 
world. Our study measured eye vergence continuously to track perceptual depth 
changes with an eye tracker in the visually-matched real world, AR, and VR 
conditions. Participants were cued to shift their gaze to targets at one of four 
different depths (0.25m, 0.75m, 1.5m, and 4.0m) and completed an alternative 
forced choice perceptual task reporting the direction of the gap in a Landolt C 
stimulus. Trial order was fully counterbalanced. Four physical monitors were 
placed at the experimental depths to present the real-world stimuli 
on-screen. We custom-mounted the pupil labs eye tracker on Microsoft HoloLens 2 
to collect the eye tracker data and represent stimuli in AR and VR. In the VR 
condition, Microsoft HoleLens 2's optics were covered with a black cloth, so no 
light entered. As expected, results showed that eye vergence angle increased 
measurably when gaze-shifting far-to-near and decreased when shifting 
near-to-far. Importantly, all three conditions induced a similar exponential 
function by which the vergence angle was modulated by depth regardless of 
whether the targets were presented on real-world displays or head-mounted 
displays. 
}, 
} 
</pre>
</div>
</div></div></body>
</html>
