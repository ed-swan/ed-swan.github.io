<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="StyleSheet" href="../css/responsive.css" type="text/css" media="all">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>J. Edward Swan II</title>
</head>
<body><div class="outer_wrapper"><div class="inner_wrapper">
<div class="header"><p>J. Edward Swan II</p></div>
<div class="menu">
<ul>
<li><a href="../index.html"><em>A</em>BOUT</a></li> •
           <li><a href="../research.html"><em>R</em>ESEARCH</a></li> •
           <li><a href="sort_date.html" class="selected"><em>P</em>UBLICATIONS</a></li> •
           <li><a href="../teaching.html"><em>T</em>EACHING</a></li> •
           <li><a href="../tutorials.html"><em>T</em>UTORIALS</a></li> •
           <li><a href="../students.html"><em>S</em>TUDENTS</a></li>
</ul>
<p> </p>
</div>
<div class="content">
<ul class="h_list">
<li><big>• <a href="class_rescat.html">Classified by Research Category</a> </big></li>
<li><big>• <a href="class_type.html">Classified by Publication Type</a> </big></li>
<li><big>• <a href="sort_date.html">Classified by Date</a> </big></li>
</ul>
<h2> Tracking Perceptual Depth Changes with Eye Vergence and Inter Pupillary                 Distance in a Virtual Reality Environment</h2>
<p class="citation"> Mohammed Safayet Arefin,  J. Edward Swan&nbsp;II,  Russell Cohen-Hoffing, and  Steven M. Thurman.  Tracking Perceptual Depth Changes with Eye Vergence and Inter Pupillary                 Distance in a Virtual Reality Environment. In <i> Vision Sciences Society Annual Meeting Abstract (VSS), Journal of Vision</i>,  22, Dec 2022.  Poster 36.408; DOI: <a target="_blank"                 href="https://doi.org/10.1167/jov.22.14.3838">10.1167/jov.22.14.3838</a>.</p>
<h3>Download</h3>
<p><a href="papers/2022_Arefin-et-al_Perceptual-Depth-Changes_VSS.pdf">[PDF]</a>&nbsp;</p>
<h3>Abstract</h3>
<p class="abstract">  Virtual reality (VR) has advanced to include eye tracking technology, allowing for exploration of novel research questions, such as how our visual system coordinates eye movements within VR and adjusts to changes in perceptual focal depth. However, because of the vergence-accommodation conflict and a lack of ground truth distance in VR, researchers typically rely on subjective measurements of depth to calibrate stimuli and interpret the vergence demand required by the visual system. Therefore, to advance methods for objectively measuring perceptual depth, we investigated whether eye-tracking-enabled VR technology can be used to estimate perceptual depth. We analyzed eye tracking data from a visual discrimination task presented on a VR display (HTC Vive Pro). Two depth-dependent human visual system components (eye vergence and interpupillary distance (IPD)) were computed independently from 24 participants' eye tracker data. Results of our study indicate that when subjects shifted their gaze from a far distance to a near distance, the vergence angle increased, and IPD decreased. As expected, we observed the opposite pattern of vergence angle and IPD when shifting gaze from a near distance to a far distance. We also found that virtual information in the retina's peripheral region was observed perceptually closer than information in the foveal or parafoveal regions. Results suggest that our method successfully estimated perceptual depth changes in VR. These results could become a new tool for researchers to track changes in perceptual depth in real time. In addition, it could allow VR and AR developers to render virtual objects with respect to the perceptual depth plane, thereby improving the user experience in VR. </p>
<a href="VSS22.bib"><h3>BibTeX</h3></a><pre>@InProceedings{VSS22, 
  author =      {Mohammed Safayet Arefin and J. Edward {Swan~II} and Russell Cohen-Hoffing
                 and Steven M. Thurman},
  title =       {Tracking Perceptual Depth Changes with Eye Vergence and Inter Pupillary
                 Distance in a Virtual Reality Environment},
  booktitle =   {Vision Sciences Society Annual Meeting Abstract (VSS), Journal of Vision},
  location =    {St. Pete Beach, Florida, USA}, 
  date =        {May 13--18},
  volume =      {22},
  page =        {3838},
  month =       {Dec}, 
  year =        2022,
  note =        {Poster 36.408; DOI: &lt;a target="_blank"
                 href="https://doi.org/10.1167/jov.22.14.3838"&gt;10.1167/jov.22.14.3838&lt;/a&gt;.},
  abstract = { 
Virtual reality (VR) has advanced to include eye tracking technology, allowing 
for exploration of novel research questions, such as how our visual system 
coordinates eye movements within VR and adjusts to changes in perceptual focal 
depth. However, because of the vergence-accommodation conflict and a lack of 
ground truth distance in VR, researchers typically rely on subjective 
measurements of depth to calibrate stimuli and interpret the vergence demand 
required by the visual system. Therefore, to advance methods for objectively 
measuring perceptual depth, we investigated whether eye-tracking-enabled VR 
technology can be used to estimate perceptual depth. We analyzed eye tracking 
data from a visual discrimination task presented on a VR display (HTC Vive 
Pro). Two depth-dependent human visual system components (eye vergence and 
interpupillary distance (IPD)) were computed independently from 24 participants' 
eye tracker data. Results of our study indicate that when subjects shifted their 
gaze from a far distance to a near distance, the vergence angle increased, and 
IPD decreased. As expected, we observed the opposite pattern of vergence angle 
and IPD when shifting gaze from a near distance to a far distance. We also found 
that virtual information in the retina's peripheral region was observed 
perceptually closer than information in the foveal or parafoveal 
regions. Results suggest that our method successfully estimated perceptual depth 
changes in VR. These results could become a new tool for researchers to track 
changes in perceptual depth in real time. In addition, it could allow VR and AR 
developers to render virtual objects with respect to the perceptual depth plane, 
thereby improving the user experience in VR. 
}, 
} 
</pre>
</div>
</div></div></body>
</html>
